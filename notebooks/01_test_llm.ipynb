{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe05059-747b-44bd-a9c6-95ff0b51c4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型路径已设置：\n",
      "大脑 (Brain): C:/Users/User/.lmstudio/models/lmstudio-community/gpt-oss-20b-GGUF/gpt-oss-20b-MXFP4.gguf\n",
      "执行者 (Executor): C:/Users/User/.lmstudio/models/Leapps/DeepAnalyze-8B-Q8_0-GGUF/deepanalyze-8b-q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import gc # 垃圾回收，用于确保模型被卸载\n",
    "\n",
    "# --- 您的新模型路径 ---\n",
    "# \"大脑\" (Brain) - 负责规划和推理\n",
    "MODEL_PATH_BRAIN = \"C:/Users/User/.lmstudio/models/lmstudio-community/gpt-oss-20b-GGUF/gpt-oss-20b-MXFP4.gguf\"\n",
    "\n",
    "# \"执行者\" (Executor) - 负责编码\n",
    "MODEL_PATH_EXECUTOR = \"C:/Users/User/.lmstudio/models/Leapps/DeepAnalyze-8B-Q8_0-GGUF/deepanalyze-8b-q8_0.gguf\"\n",
    "\n",
    "print(\"模型路径已设置：\")\n",
    "print(f\"大脑 (Brain): {MODEL_PATH_BRAIN}\")\n",
    "print(f\"执行者 (Executor): {MODEL_PATH_EXECUTOR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fb81fe-096a-46d3-9abb-a2976b399705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3060, compute capability 8.6, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060) - 11247 MiB free\n",
      "llama_model_loader: loaded meta data with 37 key-value pairs and 399 tensors from C:/Users/User/.lmstudio/models/Leapps/DeepAnalyze-8B-Q8_0-GGUF/deepanalyze-8b-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = DeepAnalyze 8B\n",
      "llama_model_loader: - kv   3:                           general.basename str              = DeepAnalyze\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   5:                            general.license str              = mit\n",
      "llama_model_loader: - kv   6:                      general.dataset.count u32              = 1\n",
      "llama_model_loader: - kv   7:                     general.dataset.0.name str              = DataScience Instruct 500K\n",
      "llama_model_loader: - kv   8:             general.dataset.0.organization str              = RUC DataLab\n",
      "llama_model_loader: - kv   9:                 general.dataset.0.repo_url str              = https://huggingface.co/RUC-DataLab/Da...\n",
      "llama_model_loader: - kv  10:                               general.tags arr[str,1]       = [\"text-generation\"]\n",
      "llama_model_loader: - kv  11:                          qwen3.block_count u32              = 36\n",
      "llama_model_loader: - kv  12:                       qwen3.context_length u32              = 131072\n",
      "llama_model_loader: - kv  13:                     qwen3.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  14:                  qwen3.feed_forward_length u32              = 12288\n",
      "llama_model_loader: - kv  15:                 qwen3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  17:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  18:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  19:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  20:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  21:                    qwen3.rope.scaling.type str              = yarn\n",
      "llama_model_loader: - kv  22:                  qwen3.rope.scaling.factor f32              = 4.000000\n",
      "llama_model_loader: - kv  23: qwen3.rope.scaling.original_context_length u32              = 32768\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始测试 DeepAnalyze 8B (快速执行者) ---\n",
      "加载模型 (全 VRAM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 151645\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                          general.file_type u32              = 7\n",
      "llama_model_loader: - type  f32:  145 tensors\n",
      "llama_model_loader: - type q8_0:  254 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 8.11 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151643 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151645 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151669 '<｜User｜>' is not marked as EOG\n",
      "load: control token: 151670 '<｜Assistant｜>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 151645 ('<｜end▁of▁sentence｜>')\n",
      "load:   - 151662 ('<|fim_pad|>')\n",
      "load:   - 151663 ('<|repo_name|>')\n",
      "load:   - 151664 ('<|file_sep|>')\n",
      "load: special tokens cache size = 38\n",
      "load: token to piece cache size = 0.9312 MB\n",
      "print_info: arch             = qwen3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 12288\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = yarn\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 0.25\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.19 B\n",
      "print_info: general.name     = DeepAnalyze 8B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<｜begin▁of▁sentence｜>'\n",
      "print_info: EOS token        = 151645 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOT token        = 151645 '<｜end▁of▁sentence｜>'\n",
      "print_info: PAD token        = 151645 '<｜end▁of▁sentence｜>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151645 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  33 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  34 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  35 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  36 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 36 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 37/37 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  7669.77 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   630.59 MiB\n",
      ".......................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 0.25\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  32: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  33: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  34: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  35: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =   576.00 MiB\n",
      "llama_kv_cache_unified: size =  576.00 MiB (  4096 cells,  36 layers,  1/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 3192\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      CUDA0 compute buffer size =   304.75 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    20.01 MiB\n",
      "llama_context: graph nodes  = 1410\n",
      "llama_context: graph splits = 2\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.dataset.0.name': 'DataScience Instruct 500K', 'general.name': 'DeepAnalyze 8B', 'general.architecture': 'qwen3', 'qwen3.attention.layer_norm_rms_epsilon': '0.000001', 'general.type': 'model', 'general.dataset.0.repo_url': 'https://huggingface.co/RUC-DataLab/DataScience-Instruct-500K', 'general.basename': 'DeepAnalyze', 'general.size_label': '8B', 'general.license': 'mit', 'general.dataset.count': '1', 'general.dataset.0.organization': 'RUC DataLab', 'tokenizer.ggml.pre': 'qwen2', 'qwen3.block_count': '36', 'qwen3.context_length': '131072', 'qwen3.embedding_length': '4096', 'qwen3.rope.scaling.type': 'yarn', 'qwen3.feed_forward_length': '12288', 'qwen3.attention.head_count': '32', 'qwen3.attention.head_count_kv': '8', 'qwen3.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'false', 'qwen3.attention.key_length': '128', 'qwen3.rope.scaling.factor': '4.000000', 'qwen3.attention.value_length': '128', 'qwen3.rope.scaling.original_context_length': '32768', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.padding_token_id': '151645', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}\\n{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false, has_execute=false) %}\\n{%- for message in messages %}\\n{%- if message['role'] == 'system' %}\\n{%- if ns.is_first_sp %}\\n{% set ns.system_prompt = ns.system_prompt + message['content'] %}\\n{% set ns.is_first_sp = false %}\\n{%- else %}\\n{% set ns.system_prompt = ns.system_prompt + '\\\\n\\\\n' + message['content'] %}\\n{%- endif %}\\n{%- endif %}\\n{%- if message['role'] == 'execute' %}\\n{% set ns.has_execute = true %}\\n{%- endif %}\\n{%- endfor %}\\n{{ bos_token }}{{ ns.system_prompt }}\\n{%- for message in messages %}\\n{% set content = message['content'] %}\\n{%- if message['role'] == 'user' %}\\n{%- set ns.is_tool = false -%}\\n{%- set ns.is_first = false -%}\\n{%- set ns.is_last_user = true -%}\\n{{\\n'<｜User｜>' + content + '<｜Assistant｜>'\\n}}\\n{%- endif %}\\n{%- if message['role'] == 'assistant' %}\\n{% if '</think>' in content %}\\n{% set content = content.split('</think>')[-1] %}\\n{% endif %}\\n{%- endif %}\\n{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}\\n{%- set ns.is_last_user = false -%}\\n{%- if ns.is_tool %}\\n{{\\n'<｜tool▁outputs▁end｜>'\\n}}\\n{%- endif %}\\n{%- set ns.is_first = false %}\\n{%- set ns.is_tool = false -%}\\n{%- set ns.is_output_first = true %}\\n{%- for tool in message['tool_calls'] %}\\n{%- if not ns.is_first %}\\n{%- if content is none %}\\n{{\\n'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'\\n}}\\n{%- else %}\\n{{\\ncontent + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'\\n}}\\n{%- endif %}\\n{%- set ns.is_first = true -%}\\n{%- else %}\\n{{\\n'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'\\n}}\\n{%- endif %}\\n{%- endfor %}\\n{{\\n'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'\\n}}\\n{%- endif %}\\n{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}\\n{%- set ns.is_last_user = false -%}\\n{%- if ns.is_tool %}\\n{{\\n'<｜tool▁outputs▁end｜>' + content + ('' if ns.has_execute else '<｜end▁of▁sentence｜>')\\n}}\\n{%- set ns.is_tool = false -%}\\n{%- else %}\\n{{\\ncontent + ('' if ns.has_execute else '<｜end▁of▁sentence｜>')\\n}}\\n{%- endif %}\\n{%- endif %}\\n{%- if message['role'] == 'tool' %}\\n{%- set ns.is_last_user = false -%}\\n{%- set ns.is_tool = true -%}\\n{%- if ns.is_output_first %}\\n{{\\n'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'\\n}}\\n{%- set ns.is_output_first = false %}\\n{%- else %}\\n{{\\n'\\\\n<｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'\\n}}\\n{%- endif %}\\n{%- endif %}\\n{%- if message['role'] == 'execute' %}\\n{%- set ns.is_last_user = false -%}\\n{{\\n'\\\\n<Execute>\\\\n' + content + '\\\\n</Execute>\\\\n'\\n}}\\n{%- endif %}\\n{%- endfor %}\\n{% if ns.is_tool %}\\n{{\\n'<｜tool▁outputs▁end｜>'\\n}}\\n{% endif %}\\n{% if add_generation_prompt and ns.is_last_user and not ns.is_tool %}\\n{{\\n'<｜Assistant｜><Analyze>'\\n}}\\n{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}\n",
      "{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true, is_last_user=false, has_execute=false) %}\n",
      "{%- for message in messages %}\n",
      "{%- if message['role'] == 'system' %}\n",
      "{%- if ns.is_first_sp %}\n",
      "{% set ns.system_prompt = ns.system_prompt + message['content'] %}\n",
      "{% set ns.is_first_sp = false %}\n",
      "{%- else %}\n",
      "{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}\n",
      "{%- endif %}\n",
      "{%- endif %}\n",
      "{%- if message['role'] == 'execute' %}\n",
      "{% set ns.has_execute = true %}\n",
      "{%- endif %}\n",
      "{%- endfor %}\n",
      "{{ bos_token }}{{ ns.system_prompt }}\n",
      "{%- for message in messages %}\n",
      "{% set content = message['content'] %}\n",
      "{%- if message['role'] == 'user' %}\n",
      "{%- set ns.is_tool = false -%}\n",
      "{%- set ns.is_first = false -%}\n",
      "{%- set ns.is_last_user = true -%}\n",
      "{{\n",
      "'<｜User｜>' + content + '<｜Assistant｜>'\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- if message['role'] == 'assistant' %}\n",
      "{% if '</think>' in content %}\n",
      "{% set content = content.split('</think>')[-1] %}\n",
      "{% endif %}\n",
      "{%- endif %}\n",
      "{%- if message['role'] == 'assistant' and message['tool_calls'] is defined and message['tool_calls'] is not none %}\n",
      "{%- set ns.is_last_user = false -%}\n",
      "{%- if ns.is_tool %}\n",
      "{{\n",
      "'<｜tool▁outputs▁end｜>'\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- set ns.is_first = false %}\n",
      "{%- set ns.is_tool = false -%}\n",
      "{%- set ns.is_output_first = true %}\n",
      "{%- for tool in message['tool_calls'] %}\n",
      "{%- if not ns.is_first %}\n",
      "{%- if content is none %}\n",
      "{{\n",
      "'<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'\n",
      "}}\n",
      "{%- else %}\n",
      "{{\n",
      "content + '<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- set ns.is_first = true -%}\n",
      "{%- else %}\n",
      "{{\n",
      "'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- endfor %}\n",
      "{{\n",
      "'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- if message['role'] == 'assistant' and (message['tool_calls'] is not defined or message['tool_calls'] is none) %}\n",
      "{%- set ns.is_last_user = false -%}\n",
      "{%- if ns.is_tool %}\n",
      "{{\n",
      "'<｜tool▁outputs▁end｜>' + content + ('' if ns.has_execute else '<｜end▁of▁sentence｜>')\n",
      "}}\n",
      "{%- set ns.is_tool = false -%}\n",
      "{%- else %}\n",
      "{{\n",
      "content + ('' if ns.has_execute else '<｜end▁of▁sentence｜>')\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- endif %}\n",
      "{%- if message['role'] == 'tool' %}\n",
      "{%- set ns.is_last_user = false -%}\n",
      "{%- set ns.is_tool = true -%}\n",
      "{%- if ns.is_output_first %}\n",
      "{{\n",
      "'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'\n",
      "}}\n",
      "{%- set ns.is_output_first = false %}\n",
      "{%- else %}\n",
      "{{\n",
      "'\\n<｜tool▁output▁begin｜>' + content + '<｜tool▁output▁end｜>'\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- endif %}\n",
      "{%- if message['role'] == 'execute' %}\n",
      "{%- set ns.is_last_user = false -%}\n",
      "{{\n",
      "'\\n<Execute>\\n' + content + '\\n</Execute>\\n'\n",
      "}}\n",
      "{%- endif %}\n",
      "{%- endfor %}\n",
      "{% if ns.is_tool %}\n",
      "{{\n",
      "'<｜tool▁outputs▁end｜>'\n",
      "}}\n",
      "{% endif %}\n",
      "{% if add_generation_prompt and ns.is_last_user and not ns.is_tool %}\n",
      "{{\n",
      "'<｜Assistant｜><Analyze>'\n",
      "}}\n",
      "{% endif %}\n",
      "Using chat eos_token: <｜end▁of▁sentence｜>\n",
      "Using chat bos_token: <｜begin▁of▁sentence｜>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完毕，耗时: 5.32 秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     183.81 ms\n",
      "llama_perf_context_print: prompt eval time =     183.65 ms /    44 tokens (    4.17 ms per token,   239.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15152.56 ms /   519 runs   (   29.20 ms per token,    34.25 tokens per second)\n",
      "llama_perf_context_print:       total time =   16052.75 ms /   563 tokens\n",
      "llama_perf_context_print:    graphs reused =        502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8B '执行者' 模型输出 ---\n",
      "We are going to use pandas to read the CSV file.\n",
      " Steps:\n",
      " 1. Import pandas.\n",
      " 2. Read the CSV file into a DataFrame.\n",
      " 3. Calculate the mean of the 'Age' column (ignoring missing values initially).\n",
      " 4. Fill the missing values in the 'Age' column with the calculated mean.\n",
      " 5. We might want to return the DataFrame or just modify it in place? The problem doesn't specify, but typically we return the modified DataFrame.\n",
      "\n",
      " However, note: the problem says \"fill any missing values in the 'Age' column with the mean age\". We are only concerned with the 'Age' column.\n",
      "\n",
      " Let's write the function accordingly.\n",
      "</Analyze>\n",
      "<Answer>\n",
      "To solve this problem, we need to read a CSV file named 'sales.csv' using pandas and fill any missing values in the 'Age' column with the mean age of the available data. Here's the step-by-step approach:\n",
      "\n",
      "1. **Import pandas**: Ensure pandas is imported to handle data operations.\n",
      "2. **Read the CSV file**: Use `pd.read_csv()` to load the data into a DataFrame.\n",
      "3. **Calculate mean age**: Compute the mean of the 'Age' column, which automatically excludes missing values.\n",
      "4. **Fill missing values**: Replace the missing values in the 'Age' column with the calculated mean.\n",
      "5. **Return the DataFrame**: The function returns the modified DataFrame with filled missing values.\n",
      "\n",
      "Here's the implementation:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def fill_missing_age():\n",
      "    # Read the CSV file into a DataFrame\n",
      "    df = pd.read_csv('sales.csv')\n",
      "    \n",
      "    # Calculate the mean age (ignores missing values by default)\n",
      "    mean_age = df['Age'].mean()\n",
      "    \n",
      "    # Fill missing values in 'Age' with the mean age\n",
      "    df['Age'].fillna(mean_age, inplace=True)\n",
      "    \n",
      "    return df\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "- **Reading the CSV**: `pd.read_csv('sales.csv')` loads the data into a DataFrame.\n",
      "- **Mean Calculation**: `df['Age'].mean()` computes the average age, automatically excluding `NaN` values.\n",
      "- **Filling Missing Values**: `fillna()` replaces `NaN` entries in the 'Age' column with the mean age. The `inplace=True` parameter modifies the DataFrame directly.\n",
      "- **Returning the DataFrame**: The function returns the DataFrame with missing ages filled, ensuring the original data structure remains intact.\n",
      "\n",
      "This approach efficiently handles missing data using pandas' built-in functions, ensuring correctness and simplicity.\n",
      "</Answer>\n",
      "--------------------\n",
      "推理耗时: 16.06 秒 (应该非常快)\n",
      "卸载 8B 模型...\n",
      "8B 模型已卸载。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 开始测试 DeepAnalyze 8B (快速执行者) ---\")\n",
    "print(\"加载模型 (全 VRAM)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    llm_executor = Llama(\n",
    "        model_path=MODEL_PATH_EXECUTOR,\n",
    "        n_gpu_layers=-1,  # -1 = 尝试将所有层加载到 GPU (全 VRAM)\n",
    "        n_ctx=4096,       # 上下文窗口大小\n",
    "        verbose=True\n",
    "    )\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"模型加载完毕，耗时: {load_time:.2f} 秒\")\n",
    "\n",
    "    # 测试推理 (编码任务)\n",
    "    prompt = \"Write a python function using pandas to read a CSV named 'sales.csv' and fill any missing values in the 'Age' column with the mean age.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    start_time = time.time()\n",
    "    output = llm_executor.create_chat_completion(messages=messages)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\n--- 8B '执行者' 模型输出 ---\")\n",
    "    print(output['choices'][0]['message']['content'])\n",
    "    print(\"--------------------\")\n",
    "    print(f\"推理耗时: {inference_time:.2f} 秒 (应该非常快)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"加载 8B 模型时出错: {e}\")\n",
    "    print(\"请检查模型路径是否正确，以及 VRAM 是否足够。\")\n",
    "\n",
    "finally:\n",
    "    # 关键步骤：卸载模型以释放 VRAM\n",
    "    if 'llm_executor' in locals():\n",
    "        print(\"卸载 8B 模型...\")\n",
    "        del llm_executor\n",
    "        gc.collect()\n",
    "        print(\"8B 模型已卸载。\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c8c7eeb-4160-4b10-b7c4-dac4d8b86d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060) - 11221 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 459 tensors from C:/Users/User/.lmstudio/models/lmstudio-community/gpt-oss-20b-GGUF/gpt-oss-20b-MXFP4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Openai_Gpt Oss 20b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = openai_gpt-oss\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 20B\n",
      "llama_model_loader: - kv   5:                        gpt-oss.block_count u32              = 24\n",
      "llama_model_loader: - kv   6:                     gpt-oss.context_length u32              = 131072\n",
      "llama_model_loader: - kv   7:                   gpt-oss.embedding_length u32              = 2880\n",
      "llama_model_loader: - kv   8:                gpt-oss.feed_forward_length u32              = 2880\n",
      "llama_model_loader: - kv   9:               gpt-oss.attention.head_count u32              = 64\n",
      "llama_model_loader: - kv  10:            gpt-oss.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  11:                     gpt-oss.rope.freq_base f32              = 150000.000000\n",
      "llama_model_loader: - kv  12:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  13:                       gpt-oss.expert_count u32              = 32\n",
      "llama_model_loader: - kv  14:                  gpt-oss.expert_used_count u32              = 4\n",
      "llama_model_loader: - kv  15:               gpt-oss.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  16:             gpt-oss.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  17:           gpt-oss.attention.sliding_window u32              = 128\n",
      "llama_model_loader: - kv  18:         gpt-oss.expert_feed_forward_length u32              = 2880\n",
      "llama_model_loader: - kv  19:                  gpt-oss.rope.scaling.type str              = yarn\n",
      "llama_model_loader: - kv  20:                gpt-oss.rope.scaling.factor f32              = 32.000000\n",
      "llama_model_loader: - kv  21: gpt-oss.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = gpt-4o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始测试 GPT-OSS 20B (大脑) ---\n",
      "加载模型 (尝试全 VRAM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,201088]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,446189]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 199998\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 200002\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 199999\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {#-\\n  In addition to the normal input...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  32:                          general.file_type u32              = 38\n",
      "llama_model_loader: - type  f32:  289 tensors\n",
      "llama_model_loader: - type q8_0:   98 tensors\n",
      "llama_model_loader: - type mxfp4:   72 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = MXFP4 MoE\n",
      "print_info: file size   = 11.27 GiB (4.63 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 200003 '<|constrain|>' is not marked as EOG\n",
      "load: control token: 200008 '<|message|>' is not marked as EOG\n",
      "load: control token: 200000 '<|reserved_200000|>' is not marked as EOG\n",
      "load: control token: 200004 '<|reserved_200004|>' is not marked as EOG\n",
      "load: control token: 200013 '<|reserved_200013|>' is not marked as EOG\n",
      "load: control token: 200009 '<|reserved_200009|>' is not marked as EOG\n",
      "load: control token: 199998 '<|startoftext|>' is not marked as EOG\n",
      "load: control token: 200014 '<|reserved_200014|>' is not marked as EOG\n",
      "load: control token: 200011 '<|reserved_200011|>' is not marked as EOG\n",
      "load: control token: 200015 '<|reserved_200015|>' is not marked as EOG\n",
      "load: control token: 200001 '<|reserved_200001|>' is not marked as EOG\n",
      "load: control token: 200005 '<|channel|>' is not marked as EOG\n",
      "load: control token: 200006 '<|start|>' is not marked as EOG\n",
      "load: control token: 200010 '<|reserved_200010|>' is not marked as EOG\n",
      "load: control token: 200016 '<|reserved_200016|>' is not marked as EOG\n",
      "load: control token: 200017 '<|reserved_200017|>' is not marked as EOG\n",
      "load: control token: 200018 '<|endofprompt|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 199999 ('<|endoftext|>')\n",
      "load:   - 200002 ('<|return|>')\n",
      "load:   - 200007 ('<|end|>')\n",
      "load:   - 200012 ('<|call|>')\n",
      "load: special_eog_ids contains both '<|return|>' and '<|call|>' tokens, removing '<|end|>' token from EOG list\n",
      "load: special tokens cache size = 21\n",
      "load: token to piece cache size = 1.3332 MB\n",
      "print_info: arch             = gpt-oss\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2880\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 64\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 128\n",
      "print_info: is_swa_any       = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 2880\n",
      "print_info: n_expert         = 32\n",
      "print_info: n_expert_used    = 4\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = yarn\n",
      "print_info: freq_base_train  = 150000.0\n",
      "print_info: freq_scale_train = 0.03125\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 20.91 B\n",
      "print_info: general.name     = Openai_Gpt Oss 20b\n",
      "print_info: n_ff_exp         = 2880\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 201088\n",
      "print_info: n_merges         = 446189\n",
      "print_info: BOS token        = 199998 '<|startoftext|>'\n",
      "print_info: EOS token        = 200002 '<|return|>'\n",
      "print_info: EOT token        = 199999 '<|endoftext|>'\n",
      "print_info: PAD token        = 199999 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 199999 '<|endoftext|>'\n",
      "print_info: EOG token        = 200002 '<|return|>'\n",
      "print_info: EOG token        = 200012 '<|call|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA0, is_swa = 1\n",
      "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 24 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 25/25 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size = 10949.38 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   586.82 MiB\n",
      "................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 150000.0\n",
      "llama_context: freq_scale    = 0.03125\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.77 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 4096 cells\n",
      "llama_kv_cache_unified: layer   0: skipped\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: skipped\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: skipped\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: skipped\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: skipped\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: skipped\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  12: skipped\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  14: skipped\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  16: skipped\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  18: skipped\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  20: skipped\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  22: skipped\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =    96.00 MiB\n",
      "llama_kv_cache_unified: size =   96.00 MiB (  4096 cells,  12 layers,  1/1 seqs), K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 4096 cells\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: skipped\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: skipped\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: skipped\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: skipped\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: skipped\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: skipped\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  13: skipped\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  15: skipped\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  17: skipped\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  19: skipped\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  21: skipped\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  23: skipped\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =    96.00 MiB\n",
      "llama_kv_cache_unified: size =   96.00 MiB (  4096 cells,  12 layers,  1/1 seqs), K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 2\n",
      "llama_context: max_nodes = 3672\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      CUDA0 compute buffer size =   559.26 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    25.64 MiB\n",
      "llama_context: graph nodes  = 1446\n",
      "llama_context: graph splits = 2\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Openai_Gpt Oss 20b', 'gpt-oss.attention.value_length': '64', 'general.architecture': 'gpt-oss', 'general.type': 'model', 'general.basename': 'openai_gpt-oss', 'gpt-oss.block_count': '24', 'general.size_label': '20B', 'tokenizer.ggml.padding_token_id': '199999', 'gpt-oss.context_length': '131072', 'gpt-oss.attention.head_count_kv': '8', 'gpt-oss.embedding_length': '2880', 'gpt-oss.feed_forward_length': '2880', 'gpt-oss.attention.head_count': '64', 'gpt-oss.expert_feed_forward_length': '2880', 'gpt-oss.rope.freq_base': '150000.000000', 'general.file_type': '38', 'tokenizer.ggml.eos_token_id': '200002', 'gpt-oss.attention.layer_norm_rms_epsilon': '0.000010', 'gpt-oss.rope.scaling.factor': '32.000000', 'gpt-oss.expert_count': '32', 'gpt-oss.expert_used_count': '4', 'gpt-oss.attention.key_length': '64', 'gpt-oss.attention.sliding_window': '128', 'tokenizer.chat_template': '{#-\\n  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\\n  following kwargs:\\n  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\\n  - \"model_identity\": A string that optionally describes the model identity.\\n  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\\n #}\\n\\n{#- Tool Definition Rendering ============================================== #}\\n{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\\n    {%- if param_spec.type == \"array\" -%}\\n        {%- if param_spec[\\'items\\'] -%}\\n            {%- if param_spec[\\'items\\'][\\'type\\'] == \"string\" -%}\\n                {{- \"string[]\" }}\\n            {%- elif param_spec[\\'items\\'][\\'type\\'] == \"number\" -%}\\n                {{- \"number[]\" }}\\n            {%- elif param_spec[\\'items\\'][\\'type\\'] == \"integer\" -%}\\n                {{- \"number[]\" }}\\n            {%- elif param_spec[\\'items\\'][\\'type\\'] == \"boolean\" -%}\\n                {{- \"boolean[]\" }}\\n            {%- else -%}\\n                {%- set inner_type = render_typescript_type(param_spec[\\'items\\'], required_params) -%}\\n                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\\n                    {{- \"any[]\" }}\\n                {%- else -%}\\n                    {{- inner_type + \"[]\" }}\\n                {%- endif -%}\\n            {%- endif -%}\\n            {%- if param_spec.nullable -%}\\n                {{- \" | null\" }}\\n            {%- endif -%}\\n        {%- else -%}\\n            {{- \"any[]\" }}\\n            {%- if param_spec.nullable -%}\\n                {{- \" | null\" }}\\n            {%- endif -%}\\n        {%- endif -%}\\n    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\\n        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\\n        {%- if param_spec.type | length > 1 -%}\\n            {{- param_spec.type | join(\" | \") }}\\n        {%- else -%}\\n            {{- param_spec.type[0] }}\\n        {%- endif -%}\\n    {%- elif param_spec.oneOf -%}\\n        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\\n        {%- set has_object_variants = false -%}\\n        {%- for variant in param_spec.oneOf -%}\\n            {%- if variant.type == \"object\" -%}\\n                {%- set has_object_variants = true -%}\\n            {%- endif -%}\\n        {%- endfor -%}\\n        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\\n            {{- \"any\" }}\\n        {%- else -%}\\n            {%- for variant in param_spec.oneOf -%}\\n                {{- render_typescript_type(variant, required_params) -}}\\n                {%- if variant.description %}\\n                    {{- \"// \" + variant.description }}\\n                {%- endif -%}\\n                {%- if variant.default is defined %}\\n                    {{ \"// default: \" + variant.default|tojson }}\\n                {%- endif -%}\\n                {%- if not loop.last %}\\n                    {{- \" | \" }}\\n                {% endif -%}\\n            {%- endfor -%}\\n        {%- endif -%}\\n    {%- elif param_spec.type == \"string\" -%}\\n        {%- if param_spec.enum -%}\\n            {{- \\'\"\\' + param_spec.enum|join(\\'\" | \"\\') + \\'\"\\' -}}\\n        {%- else -%}\\n            {{- \"string\" }}\\n            {%- if param_spec.nullable %}\\n                {{- \" | null\" }}\\n            {%- endif -%}\\n        {%- endif -%}\\n    {%- elif param_spec.type == \"number\" -%}\\n        {{- \"number\" }}\\n    {%- elif param_spec.type == \"integer\" -%}\\n        {{- \"number\" }}\\n    {%- elif param_spec.type == \"boolean\" -%}\\n        {{- \"boolean\" }}\\n\\n    {%- elif param_spec.type == \"object\" -%}\\n        {%- if param_spec.properties -%}\\n            {{- \"{\\n\" }}\\n            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\\n                {{- prop_name -}}\\n                {%- if prop_name not in (param_spec.required or []) -%}\\n                    {{- \"?\" }}\\n                {%- endif -%}\\n                {{- \": \" }}\\n                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\\n                {%- if not loop.last -%}\\n                    {{-\", \" }}\\n                {%- endif -%}\\n            {%- endfor -%}\\n            {{- \"}\" }}\\n        {%- else -%}\\n            {{- \"object\" }}\\n        {%- endif -%}\\n    {%- else -%}\\n        {{- \"any\" }}\\n    {%- endif -%}\\n{%- endmacro -%}\\n\\n{%- macro render_tool_namespace(namespace_name, tools) -%}\\n    {{- \"## \" + namespace_name + \"\\n\\n\" }}\\n    {{- \"namespace \" + namespace_name + \" {\\n\\n\" }}\\n    {%- for tool in tools %}\\n        {%- set tool = tool.function %}\\n        {{- \"// \" + tool.description + \"\\n\" }}\\n        {{- \"type \"+ tool.name + \" = \" }}\\n        {%- if tool.parameters and tool.parameters.properties %}\\n            {{- \"(_: {\\n\" }}\\n            {%- for param_name, param_spec in tool.parameters.properties.items() %}\\n                {%- if param_spec.description %}\\n                    {{- \"// \" + param_spec.description + \"\\n\" }}\\n                {%- endif %}\\n                {{- param_name }}\\n                {%- if param_name not in (tool.parameters.required or []) -%}\\n                    {{- \"?\" }}\\n                {%- endif -%}\\n                {{- \": \" }}\\n                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\\n                {%- if param_spec.default is defined -%}\\n                    {%- if param_spec.enum %}\\n                        {{- \", // default: \" + param_spec.default }}\\n                    {%- elif param_spec.oneOf %}\\n                        {{- \"// default: \" + param_spec.default }}\\n                    {%- else %}\\n                        {{- \", // default: \" + param_spec.default|tojson }}\\n                    {%- endif -%}\\n                {%- endif -%}\\n                {%- if not loop.last %}\\n                    {{- \",\\n\" }}\\n                {%- else %}\\n                    {{- \"\\n\" }}\\n                {%- endif -%}\\n            {%- endfor %}\\n            {{- \"}) => any;\\n\\n\" }}\\n        {%- else -%}\\n            {{- \"() => any;\\n\\n\" }}\\n        {%- endif -%}\\n    {%- endfor %}\\n    {{- \"} // namespace \" + namespace_name }}\\n{%- endmacro -%}\\n\\n{%- macro render_builtin_tools(browser_tool, python_tool) -%}\\n    {%- if browser_tool %}\\n        {{- \"## browser\\n\\n\" }}\\n        {{- \"// Tool for browsing.\\n\" }}\\n        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\\n\" }}\\n        {{- \"// Cite information from the tool using the following format:\\n\" }}\\n        {{- \"// `【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`.\\n\" }}\\n        {{- \"// Do not quote more than 10 words directly from the tool output.\\n\" }}\\n        {{- \"// sources=web (default: web)\\n\" }}\\n        {{- \"namespace browser {\\n\\n\" }}\\n        {{- \"// Searches for information related to `query` and displays `topn` results.\\n\" }}\\n        {{- \"type search = (_: {\\n\" }}\\n        {{- \"query: string,\\n\" }}\\n        {{- \"topn?: number, // default: 10\\n\" }}\\n        {{- \"source?: string,\\n\" }}\\n        {{- \"}) => any;\\n\\n\" }}\\n        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\\n\" }}\\n        {{- \"// Valid link ids are displayed with the formatting: `【{id}†.*】`.\\n\" }}\\n        {{- \"// If `cursor` is not provided, the most recent page is implied.\\n\" }}\\n        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\\n\" }}\\n        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\\n\" }}\\n        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\\n\" }}\\n        {{- \"type open = (_: {\\n\" }}\\n        {{- \"id?: number | string, // default: -1\\n\" }}\\n        {{- \"cursor?: number, // default: -1\\n\" }}\\n        {{- \"loc?: number, // default: -1\\n\" }}\\n        {{- \"num_lines?: number, // default: -1\\n\" }}\\n        {{- \"view_source?: boolean, // default: false\\n\" }}\\n        {{- \"source?: string,\\n\" }}\\n        {{- \"}) => any;\\n\\n\" }}\\n        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\\n\" }}\\n        {{- \"type find = (_: {\\n\" }}\\n        {{- \"pattern: string,\\n\" }}\\n        {{- \"cursor?: number, // default: -1\\n\" }}\\n        {{- \"}) => any;\\n\\n\" }}\\n        {{- \"} // namespace browser\\n\\n\" }}\\n    {%- endif -%}\\n\\n    {%- if python_tool %}\\n        {{- \"## python\\n\\n\" }}\\n        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\\n\\n\" }}\\n        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at \\'/mnt/data\\' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\\n\\n\" }}\\n    {%- endif -%}\\n{%- endmacro -%}\\n\\n{#- System Message Construction ============================================ #}\\n{%- macro build_system_message() -%}\\n    {%- if model_identity is not defined %}\\n        {%- set model_identity = \"You are ChatGPT, a large language model trained by OpenAI.\" %}\\n    {%- endif %}\\n    {{- model_identity + \"\\n\" }}\\n    {{- \"Knowledge cutoff: 2024-06\\n\" }}\\n    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\\n\\n\" }}\\n    {%- if reasoning_effort is not defined %}\\n        {%- set reasoning_effort = \"medium\" %}\\n    {%- endif %}\\n    {{- \"Reasoning: \" + reasoning_effort + \"\\n\\n\" }}\\n    {%- if builtin_tools %}\\n        {{- \"# Tools\\n\\n\" }}\\n        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\\n        {%- for tool in builtin_tools %}\\n            {%- if tool == \"browser\" %}\\n                {%- set available_builtin_tools.browser = true %}\\n            {%- elif tool == \"python\" %}\\n                {%- set available_builtin_tools.python = true %}\\n            {%- endif %}\\n        {%- endfor %}\\n        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\\n    {%- endif -%}\\n    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\\n    {%- if tools -%}\\n        {{- \"\\nCalls to these tools must go to the commentary channel: \\'functions\\'.\" }}\\n    {%- endif -%}\\n{%- endmacro -%}\\n\\n{#- Main Template Logic ================================================= #}\\n{#- Set defaults #}\\n\\n{#- Render system message #}\\n{{- \"<|start|>system<|message|>\" }}\\n{{- build_system_message() }}\\n{{- \"<|end|>\" }}\\n\\n{#- Extract developer message #}\\n{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\\n    {%- set developer_message = messages[0].content %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set developer_message = \"\" %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{#- Render developer message #}\\n{%- if developer_message or tools %}\\n    {{- \"<|start|>developer<|message|>\" }}\\n    {%- if developer_message %}\\n        {{- \"# Instructions\\n\\n\" }}\\n        {{- developer_message }}\\n    {%- endif %}\\n    {%- if tools -%}\\n        {{- \"\\n\\n\" }}\\n        {{- \"# Tools\\n\\n\" }}\\n        {{- render_tool_namespace(\"functions\", tools) }}\\n    {%- endif -%}\\n    {{- \"<|end|>\" }}\\n{%- endif %}\\n\\n{#- Render messages #}\\n{%- set last_tool_call = namespace(name=none) %}\\n{%- for message in loop_messages -%}\\n    {#- At this point only assistant/user/tool messages should remain #}\\n    {%- if message.role == \\'assistant\\' -%}\\n        {#- Checks to ensure the messages are being passed in the format we expect #}\\n        {%- if \"content\" in message %}\\n            {%- if \"<|channel|>analysis<|message|>\" in message.content or \"<|channel|>final<|message|>\" in message.content %}\\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between \\'<|message|>\\' and \\'<|end|>\\') in the \\'thinking\\' field, and final messages (the string between \\'<|message|>\\' and \\'<|end|>\\') in the \\'content\\' field.\") }}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if \"thinking\" in message %}\\n            {%- if \"<|channel|>analysis<|message|>\" in message.thinking or \"<|channel|>final<|message|>\" in message.thinking %}\\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between \\'<|message|>\\' and \\'<|end|>\\') in the \\'thinking\\' field, and final messages (the string between \\'<|message|>\\' and \\'<|end|>\\') in the \\'content\\' field.\") }}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if \"tool_calls\" in message %}\\n            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\\n            {#- in \"tool\" messages from the most recent assistant tool call name #}\\n            {%- set tool_call = message.tool_calls[0] %}\\n            {%- if tool_call.function %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {%- if message.content and message.thinking %}\\n                {{- raise_exception(\"Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.\") }}\\n            {%- elif message.content %}\\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\\n            {%- elif message.thinking %}\\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\\n            {%- endif %}\\n            {{- \"<|start|>assistant to=\" }}\\n            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary \" }}\\n            {{- (tool_call.content_type if tool_call.content_type is defined else \"json\") + \"<|message|>\" }}\\n            {{- tool_call.arguments|tojson }}\\n            {{- \"<|call|>\" }}\\n            {%- set last_tool_call.name = tool_call.name %}\\n        {%- elif loop.last and not add_generation_prompt %}\\n            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\\n            {#- This is a situation that should only occur in training, never in inference. #}\\n            {%- if \"thinking\" in message %}\\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\\n            {%- endif %}\\n            {#- <|return|> indicates the end of generation, but <|end|> does not #}\\n            {#- <|return|> should never be an input to the model, but we include it as the final token #}\\n            {#- when training, so the model learns to emit it. #}\\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\\n        {%- else %}\\n            {#- CoT is dropped during all previous turns, so we never render it for inference #}\\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\\n            {%- set last_tool_call.name = none %}\\n        {%- endif %}\\n    {%- elif message.role == \\'tool\\' -%}\\n        {%- if last_tool_call.name is none %}\\n            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\\n        {%- endif %}\\n        {{- \"<|start|>functions.\" + last_tool_call.name }}\\n        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\\n    {%- elif message.role == \\'user\\' -%}\\n        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\\n    {%- endif -%}\\n{%- endfor -%}\\n\\n{#- Generation prompt #}\\n{%- if add_generation_prompt -%}\\n<|start|>assistant\\n{%- endif -%}', 'gpt-oss.rope.scaling.type': 'yarn', 'gpt-oss.rope.scaling.original_context_length': '4096', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'gpt-4o', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '199998'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {#-\n",
      "  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\n",
      "  following kwargs:\n",
      "  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\n",
      "  - \"model_identity\": A string that optionally describes the model identity.\n",
      "  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\n",
      " #}\n",
      "\n",
      "{#- Tool Definition Rendering ============================================== #}\n",
      "{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\n",
      "    {%- if param_spec.type == \"array\" -%}\n",
      "        {%- if param_spec['items'] -%}\n",
      "            {%- if param_spec['items']['type'] == \"string\" -%}\n",
      "                {{- \"string[]\" }}\n",
      "            {%- elif param_spec['items']['type'] == \"number\" -%}\n",
      "                {{- \"number[]\" }}\n",
      "            {%- elif param_spec['items']['type'] == \"integer\" -%}\n",
      "                {{- \"number[]\" }}\n",
      "            {%- elif param_spec['items']['type'] == \"boolean\" -%}\n",
      "                {{- \"boolean[]\" }}\n",
      "            {%- else -%}\n",
      "                {%- set inner_type = render_typescript_type(param_spec['items'], required_params) -%}\n",
      "                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\n",
      "                    {{- \"any[]\" }}\n",
      "                {%- else -%}\n",
      "                    {{- inner_type + \"[]\" }}\n",
      "                {%- endif -%}\n",
      "            {%- endif -%}\n",
      "            {%- if param_spec.nullable -%}\n",
      "                {{- \" | null\" }}\n",
      "            {%- endif -%}\n",
      "        {%- else -%}\n",
      "            {{- \"any[]\" }}\n",
      "            {%- if param_spec.nullable -%}\n",
      "                {{- \" | null\" }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\n",
      "        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\n",
      "        {%- if param_spec.type | length > 1 -%}\n",
      "            {{- param_spec.type | join(\" | \") }}\n",
      "        {%- else -%}\n",
      "            {{- param_spec.type[0] }}\n",
      "        {%- endif -%}\n",
      "    {%- elif param_spec.oneOf -%}\n",
      "        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\n",
      "        {%- set has_object_variants = false -%}\n",
      "        {%- for variant in param_spec.oneOf -%}\n",
      "            {%- if variant.type == \"object\" -%}\n",
      "                {%- set has_object_variants = true -%}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\n",
      "            {{- \"any\" }}\n",
      "        {%- else -%}\n",
      "            {%- for variant in param_spec.oneOf -%}\n",
      "                {{- render_typescript_type(variant, required_params) -}}\n",
      "                {%- if variant.description %}\n",
      "                    {{- \"// \" + variant.description }}\n",
      "                {%- endif -%}\n",
      "                {%- if variant.default is defined %}\n",
      "                    {{ \"// default: \" + variant.default|tojson }}\n",
      "                {%- endif -%}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \" | \" }}\n",
      "                {% endif -%}\n",
      "            {%- endfor -%}\n",
      "        {%- endif -%}\n",
      "    {%- elif param_spec.type == \"string\" -%}\n",
      "        {%- if param_spec.enum -%}\n",
      "            {{- '\"' + param_spec.enum|join('\" | \"') + '\"' -}}\n",
      "        {%- else -%}\n",
      "            {{- \"string\" }}\n",
      "            {%- if param_spec.nullable %}\n",
      "                {{- \" | null\" }}\n",
      "            {%- endif -%}\n",
      "        {%- endif -%}\n",
      "    {%- elif param_spec.type == \"number\" -%}\n",
      "        {{- \"number\" }}\n",
      "    {%- elif param_spec.type == \"integer\" -%}\n",
      "        {{- \"number\" }}\n",
      "    {%- elif param_spec.type == \"boolean\" -%}\n",
      "        {{- \"boolean\" }}\n",
      "\n",
      "    {%- elif param_spec.type == \"object\" -%}\n",
      "        {%- if param_spec.properties -%}\n",
      "            {{- \"{\n",
      "\" }}\n",
      "            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\n",
      "                {{- prop_name -}}\n",
      "                {%- if prop_name not in (param_spec.required or []) -%}\n",
      "                    {{- \"?\" }}\n",
      "                {%- endif -%}\n",
      "                {{- \": \" }}\n",
      "                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\n",
      "                {%- if not loop.last -%}\n",
      "                    {{-\", \" }}\n",
      "                {%- endif -%}\n",
      "            {%- endfor -%}\n",
      "            {{- \"}\" }}\n",
      "        {%- else -%}\n",
      "            {{- \"object\" }}\n",
      "        {%- endif -%}\n",
      "    {%- else -%}\n",
      "        {{- \"any\" }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro render_tool_namespace(namespace_name, tools) -%}\n",
      "    {{- \"## \" + namespace_name + \"\n",
      "\n",
      "\" }}\n",
      "    {{- \"namespace \" + namespace_name + \" {\n",
      "\n",
      "\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {%- set tool = tool.function %}\n",
      "        {{- \"// \" + tool.description + \"\n",
      "\" }}\n",
      "        {{- \"type \"+ tool.name + \" = \" }}\n",
      "        {%- if tool.parameters and tool.parameters.properties %}\n",
      "            {{- \"(_: {\n",
      "\" }}\n",
      "            {%- for param_name, param_spec in tool.parameters.properties.items() %}\n",
      "                {%- if param_spec.description %}\n",
      "                    {{- \"// \" + param_spec.description + \"\n",
      "\" }}\n",
      "                {%- endif %}\n",
      "                {{- param_name }}\n",
      "                {%- if param_name not in (tool.parameters.required or []) -%}\n",
      "                    {{- \"?\" }}\n",
      "                {%- endif -%}\n",
      "                {{- \": \" }}\n",
      "                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\n",
      "                {%- if param_spec.default is defined -%}\n",
      "                    {%- if param_spec.enum %}\n",
      "                        {{- \", // default: \" + param_spec.default }}\n",
      "                    {%- elif param_spec.oneOf %}\n",
      "                        {{- \"// default: \" + param_spec.default }}\n",
      "                    {%- else %}\n",
      "                        {{- \", // default: \" + param_spec.default|tojson }}\n",
      "                    {%- endif -%}\n",
      "                {%- endif -%}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \",\n",
      "\" }}\n",
      "                {%- else %}\n",
      "                    {{- \"\n",
      "\" }}\n",
      "                {%- endif -%}\n",
      "            {%- endfor %}\n",
      "            {{- \"}) => any;\n",
      "\n",
      "\" }}\n",
      "        {%- else -%}\n",
      "            {{- \"() => any;\n",
      "\n",
      "\" }}\n",
      "        {%- endif -%}\n",
      "    {%- endfor %}\n",
      "    {{- \"} // namespace \" + namespace_name }}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{%- macro render_builtin_tools(browser_tool, python_tool) -%}\n",
      "    {%- if browser_tool %}\n",
      "        {{- \"## browser\n",
      "\n",
      "\" }}\n",
      "        {{- \"// Tool for browsing.\n",
      "\" }}\n",
      "        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\n",
      "\" }}\n",
      "        {{- \"// Cite information from the tool using the following format:\n",
      "\" }}\n",
      "        {{- \"// `【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`.\n",
      "\" }}\n",
      "        {{- \"// Do not quote more than 10 words directly from the tool output.\n",
      "\" }}\n",
      "        {{- \"// sources=web (default: web)\n",
      "\" }}\n",
      "        {{- \"namespace browser {\n",
      "\n",
      "\" }}\n",
      "        {{- \"// Searches for information related to `query` and displays `topn` results.\n",
      "\" }}\n",
      "        {{- \"type search = (_: {\n",
      "\" }}\n",
      "        {{- \"query: string,\n",
      "\" }}\n",
      "        {{- \"topn?: number, // default: 10\n",
      "\" }}\n",
      "        {{- \"source?: string,\n",
      "\" }}\n",
      "        {{- \"}) => any;\n",
      "\n",
      "\" }}\n",
      "        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\n",
      "\" }}\n",
      "        {{- \"// Valid link ids are displayed with the formatting: `【{id}†.*】`.\n",
      "\" }}\n",
      "        {{- \"// If `cursor` is not provided, the most recent page is implied.\n",
      "\" }}\n",
      "        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\n",
      "\" }}\n",
      "        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\n",
      "\" }}\n",
      "        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\n",
      "\" }}\n",
      "        {{- \"type open = (_: {\n",
      "\" }}\n",
      "        {{- \"id?: number | string, // default: -1\n",
      "\" }}\n",
      "        {{- \"cursor?: number, // default: -1\n",
      "\" }}\n",
      "        {{- \"loc?: number, // default: -1\n",
      "\" }}\n",
      "        {{- \"num_lines?: number, // default: -1\n",
      "\" }}\n",
      "        {{- \"view_source?: boolean, // default: false\n",
      "\" }}\n",
      "        {{- \"source?: string,\n",
      "\" }}\n",
      "        {{- \"}) => any;\n",
      "\n",
      "\" }}\n",
      "        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\n",
      "\" }}\n",
      "        {{- \"type find = (_: {\n",
      "\" }}\n",
      "        {{- \"pattern: string,\n",
      "\" }}\n",
      "        {{- \"cursor?: number, // default: -1\n",
      "\" }}\n",
      "        {{- \"}) => any;\n",
      "\n",
      "\" }}\n",
      "        {{- \"} // namespace browser\n",
      "\n",
      "\" }}\n",
      "    {%- endif -%}\n",
      "\n",
      "    {%- if python_tool %}\n",
      "        {{- \"## python\n",
      "\n",
      "\" }}\n",
      "        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\n",
      "\n",
      "\" }}\n",
      "        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\n",
      "\n",
      "\" }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{#- System Message Construction ============================================ #}\n",
      "{%- macro build_system_message() -%}\n",
      "    {%- if model_identity is not defined %}\n",
      "        {%- set model_identity = \"You are ChatGPT, a large language model trained by OpenAI.\" %}\n",
      "    {%- endif %}\n",
      "    {{- model_identity + \"\n",
      "\" }}\n",
      "    {{- \"Knowledge cutoff: 2024-06\n",
      "\" }}\n",
      "    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\n",
      "\n",
      "\" }}\n",
      "    {%- if reasoning_effort is not defined %}\n",
      "        {%- set reasoning_effort = \"medium\" %}\n",
      "    {%- endif %}\n",
      "    {{- \"Reasoning: \" + reasoning_effort + \"\n",
      "\n",
      "\" }}\n",
      "    {%- if builtin_tools %}\n",
      "        {{- \"# Tools\n",
      "\n",
      "\" }}\n",
      "        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\n",
      "        {%- for tool in builtin_tools %}\n",
      "            {%- if tool == \"browser\" %}\n",
      "                {%- set available_builtin_tools.browser = true %}\n",
      "            {%- elif tool == \"python\" %}\n",
      "                {%- set available_builtin_tools.python = true %}\n",
      "            {%- endif %}\n",
      "        {%- endfor %}\n",
      "        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\n",
      "    {%- endif -%}\n",
      "    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\n",
      "    {%- if tools -%}\n",
      "        {{- \"\n",
      "Calls to these tools must go to the commentary channel: 'functions'.\" }}\n",
      "    {%- endif -%}\n",
      "{%- endmacro -%}\n",
      "\n",
      "{#- Main Template Logic ================================================= #}\n",
      "{#- Set defaults #}\n",
      "\n",
      "{#- Render system message #}\n",
      "{{- \"<|start|>system<|message|>\" }}\n",
      "{{- build_system_message() }}\n",
      "{{- \"<|end|>\" }}\n",
      "\n",
      "{#- Extract developer message #}\n",
      "{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\n",
      "    {%- set developer_message = messages[0].content %}\n",
      "    {%- set loop_messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set developer_message = \"\" %}\n",
      "    {%- set loop_messages = messages %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- Render developer message #}\n",
      "{%- if developer_message or tools %}\n",
      "    {{- \"<|start|>developer<|message|>\" }}\n",
      "    {%- if developer_message %}\n",
      "        {{- \"# Instructions\n",
      "\n",
      "\" }}\n",
      "        {{- developer_message }}\n",
      "    {%- endif %}\n",
      "    {%- if tools -%}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "        {{- \"# Tools\n",
      "\n",
      "\" }}\n",
      "        {{- render_tool_namespace(\"functions\", tools) }}\n",
      "    {%- endif -%}\n",
      "    {{- \"<|end|>\" }}\n",
      "{%- endif %}\n",
      "\n",
      "{#- Render messages #}\n",
      "{%- set last_tool_call = namespace(name=none) %}\n",
      "{%- for message in loop_messages -%}\n",
      "    {#- At this point only assistant/user/tool messages should remain #}\n",
      "    {%- if message.role == 'assistant' -%}\n",
      "        {#- Checks to ensure the messages are being passed in the format we expect #}\n",
      "        {%- if \"content\" in message %}\n",
      "            {%- if \"<|channel|>analysis<|message|>\" in message.content or \"<|channel|>final<|message|>\" in message.content %}\n",
      "                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.\") }}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if \"thinking\" in message %}\n",
      "            {%- if \"<|channel|>analysis<|message|>\" in message.thinking or \"<|channel|>final<|message|>\" in message.thinking %}\n",
      "                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.\") }}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if \"tool_calls\" in message %}\n",
      "            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\n",
      "            {#- in \"tool\" messages from the most recent assistant tool call name #}\n",
      "            {%- set tool_call = message.tool_calls[0] %}\n",
      "            {%- if tool_call.function %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {%- if message.content and message.thinking %}\n",
      "                {{- raise_exception(\"Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.\") }}\n",
      "            {%- elif message.content %}\n",
      "                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\n",
      "            {%- elif message.thinking %}\n",
      "                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n",
      "            {%- endif %}\n",
      "            {{- \"<|start|>assistant to=\" }}\n",
      "            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary \" }}\n",
      "            {{- (tool_call.content_type if tool_call.content_type is defined else \"json\") + \"<|message|>\" }}\n",
      "            {{- tool_call.arguments|tojson }}\n",
      "            {{- \"<|call|>\" }}\n",
      "            {%- set last_tool_call.name = tool_call.name %}\n",
      "        {%- elif loop.last and not add_generation_prompt %}\n",
      "            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\n",
      "            {#- This is a situation that should only occur in training, never in inference. #}\n",
      "            {%- if \"thinking\" in message %}\n",
      "                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n",
      "            {%- endif %}\n",
      "            {#- <|return|> indicates the end of generation, but <|end|> does not #}\n",
      "            {#- <|return|> should never be an input to the model, but we include it as the final token #}\n",
      "            {#- when training, so the model learns to emit it. #}\n",
      "            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\n",
      "        {%- else %}\n",
      "            {#- CoT is dropped during all previous turns, so we never render it for inference #}\n",
      "            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\n",
      "            {%- set last_tool_call.name = none %}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == 'tool' -%}\n",
      "        {%- if last_tool_call.name is none %}\n",
      "            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|start|>functions.\" + last_tool_call.name }}\n",
      "        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\n",
      "    {%- elif message.role == 'user' -%}\n",
      "        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "\n",
      "{#- Generation prompt #}\n",
      "{%- if add_generation_prompt -%}\n",
      "<|start|>assistant\n",
      "{%- endif -%}\n",
      "Using chat eos_token: <|return|>\n",
      "Using chat bos_token: <|startoftext|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完毕，耗时: 9.02 秒\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     793.15 ms\n",
      "llama_perf_context_print: prompt eval time =     792.93 ms /   105 tokens (    7.55 ms per token,   132.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =  180540.20 ms /  2272 runs   (   79.46 ms per token,    12.58 tokens per second)\n",
      "llama_perf_context_print:       total time =  189464.48 ms /  2377 tokens\n",
      "llama_perf_context_print:    graphs reused =       2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 20B '大脑' 模型输出 ---\n",
      "<|channel|>analysis<|message|>We need to outline CRISP-DM steps for a bank credit risk model, focusing on fairness and compliance. Provide details for each step: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment. For each step, highlight fairness and compliance aspects: regulatory requirements (e.g., Basel III, GDPR, Equal Credit Opportunity Act), data privacy, bias mitigation, explainability, auditability, documentation, model monitoring, etc. Provide actionable items, best practices, tools. Provide a structured outline. Maybe include substeps. Provide references to frameworks like AI Fairness 360, Model Card, etc. Provide compliance checklists. Provide fairness metrics: disparate impact, equal opportunity, etc. Provide mitigation strategies: reweighting, adversarial debiasing, fairness constraints. Provide model monitoring for drift. Provide documentation: model card, data sheet. Provide audit trails. Provide governance: model risk management, data governance, ethics board. Provide compliance: KYC, AML, privacy. Provide data handling: pseudonymization, encryption. Provide evaluation: fairness metrics, performance metrics, regulatory thresholds. Provide deployment: explainability, human-in-the-loop, monitoring. Provide post-deployment: model performance, fairness drift, regulatory reporting. Provide references to regulations: GDPR, CCPA, Basel III, Dodd-Frank, etc. Provide a step-by-step outline. Provide bullet points. Provide a summary. Provide a table maybe. Provide a risk matrix. Provide a timeline. Provide a checklist. Provide a sample workflow. Provide a note on stakeholder communication. Provide a note on documentation. Provide a note on model risk management. Provide a note on model governance. Provide a note on data lineage. Provide a note on model versioning. Provide a note on model interpretability. Provide a note on fairness testing. Provide a note on compliance testing. Provide a note on audit logs. Provide a note on model performance. Provide a note on model monitoring. Provide a note on model retraining. Provide a note on model rollback. Provide a note on model documentation. Provide a note on model risk assessment. Provide a note on model approval. Provide a note on model sign-off. Provide a note on model lifecycle. Provide a note on model risk management. Provide a note on model governance. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note on model risk. Provide a note. Enough.<|end|><|start|>assistant<|channel|>final<|message|>## CRISP‑DM for a Bank Credit‑Risk Model  \n",
      "*With a built‑in focus on fairness and regulatory compliance*\n",
      "\n",
      "| Phase | Typical Activities | Fairness / Compliance Touchpoints | Practical Tips / Tools |\n",
      "|-------|--------------------|-----------------------------------|------------------------|\n",
      "| **1. Business Understanding** | • Define the credit‑risk objective (e.g., default probability, loss‑given‑default). <br>• Identify key stakeholders (credit risk, compliance, legal, data governance). <br>• Map regulatory requirements (Basel III, GDPR, ECA, AML, CCPA, etc.). | • **Fairness**: Clarify which protected classes (race, gender, age, etc.) must be protected. <br>• **Compliance**: Document data‑subject rights, KYC/AML obligations, audit‑ready metrics. | • Create a *Business Requirements Document* (BRD) that includes a “Fairness & Compliance” section. <br>• Use a **Regulatory Impact Matrix** to trace each requirement to model outputs. |\n",
      "| **2. Data Understanding** | • Inventory data sources (applicant data, transaction history, external credit bureaus). <br>• Perform exploratory data analysis (EDA). <br>• Assess data quality, missingness, and representativeness. | • **Fairness**: Check for disparate representation of protected groups. <br>• **Compliance**: Verify that data is collected under lawful bases (consent, legitimate interest, contractual necessity). | • Use **Data Sheets for Datasets** (Karpathy, 2020) to document provenance, collection methods, and privacy controls. <br>• Run *Fairness‑Audit* scripts (AIF360, Fairlearn) to compute baseline disparate impact. |\n",
      "| **3. Data Preparation** | • Clean, transform, and engineer features. <br>• Handle missing values, outliers, and scaling. <br>• Split data into training/validation/test sets. | • **Fairness**: Apply bias‑mitigation preprocessing (reweighting, resampling, adversarial debiasing). <br>• **Compliance**: Ensure pseudonymization, encryption, and access controls during preprocessing. | • Version‑control data pipelines (MLflow, Kedro). <br>• Store audit logs of data transformations. |\n",
      "| **4. Modeling** | • Select algorithms (logistic regression, gradient‑boosted trees, neural nets). <br>• Train models, tune hyper‑parameters. <br>• Generate interpretability artifacts (SHAP, LIME). | • **Fairness**: Incorporate fairness constraints (e.g., equal opportunity, demographic parity) into the objective or post‑process predictions. <br>• **Compliance**: Use *Explainable AI* (XAI) to satisfy “right to explanation” under GDPR. | • Use **Model Cards** (Mitchell et al., 2019) to record model purpose, performance, and fairness metrics. <br>• Store model artifacts in a secure registry (MLflow, DVC). |\n",
      "| **5. Evaluation** | • Evaluate predictive performance (AUC‑ROC, KS, Gini). <br>• Evaluate fairness metrics (disparate impact, equalized odds, calibration across groups). <br>• Perform stress‑testing and scenario analysis. | • **Fairness**: Verify that fairness constraints are met across all protected groups. <br>• **Compliance**: Validate that model meets regulatory thresholds (e.g., Basel III capital adequacy). | • Create a *Model Evaluation Report* that includes both performance and fairness tables. <br>• Use *Model Risk Assessment* templates to capture risk scores. |\n",
      "| **6. Deployment** | • Integrate model into credit‑decision workflow (API, batch). <br>• Set up monitoring dashboards. <br>• Establish rollback procedures. | • **Fairness**: Continuously monitor fairness metrics in production; trigger alerts on drift. <br>• **Compliance**: Log all decisions, maintain audit trails, and provide explanations to regulators. | • Deploy via containerized services (Docker/Kubernetes). <br>• Use *Model Monitoring* tools (Seldon, Evidently AI) to track performance & fairness. |\n",
      "| **7. Post‑Deployment Monitoring & Maintenance** | • Track model drift, performance degradation. <br>• Schedule periodic retraining. <br>• Conduct model reviews. | • **Fairness**: Re‑evaluate fairness after retraining; update mitigation strategies. <br>• **Compliance**: Keep records of all changes for audit; update regulatory filings. | • Automate monitoring with *ModelOps* pipelines. <br>• Store change logs in a *Model Governance* system (e.g., ModelHub). |\n",
      "\n",
      "---\n",
      "\n",
      "### Detailed Fairness & Compliance Checklist\n",
      "\n",
      "| Item | Description | Where to Apply |\n",
      "|------|-------------|----------------|\n",
      "| **Regulatory Mapping** | Map every regulatory requirement to a model component (data, algorithm, output). | Business Understanding |\n",
      "| **Data Governance** | Data lineage, consent management, pseudonymization, encryption. | Data Understanding / Preparation |\n",
      "| **Bias Detection** | Baseline disparate impact, equalized odds, calibration gaps. | Data Understanding / Evaluation |\n",
      "| **Bias Mitigation** | Reweighting, resampling, adversarial debiasing, fairness constraints. | Data Preparation / Modeling |\n",
      "| **Explainability** | SHAP, LIME, rule‑based explanations, decision‑tree visualizations. | Modeling / Deployment |\n",
      "| **Audit Trail** | Immutable logs of data access, model training, predictions, decisions. | Deployment / Monitoring |\n",
      "| **Model Documentation** | Model Card, Data Sheet, Risk Assessment, Version History. | All phases |\n",
      "| **Model Risk Management** | Risk score, capital allocation, stress tests. | Evaluation / Deployment |\n",
      "| **Governance Board** | Regular reviews by risk, compliance, legal, and ethics committees. | All phases |\n",
      "| **Regulatory Reporting** | Periodic model performance and fairness reports to regulators. | Deployment / Monitoring |\n",
      "| **Human‑in‑the‑Loop** | Escalation paths for borderline cases, manual overrides. | Deployment |\n",
      "\n",
      "---\n",
      "\n",
      "### Practical Workflow Example\n",
      "\n",
      "1. **Kick‑off Meeting**  \n",
      "   *Participants*: Credit Risk Lead, Data Scientist, Compliance Officer, Legal Counsel, Data Governance Lead.  \n",
      "   *Deliverable*: BRD with a “Fairness & Compliance” section.\n",
      "\n",
      "2. **Data Inventory & Privacy Impact Assessment (PIA)**  \n",
      "   *Tools*: Data Catalog, GDPR‑PIA templates.  \n",
      "   *Deliverable*: Data Sheet + PIA report.\n",
      "\n",
      "3. **Pre‑processing Pipeline**  \n",
      "   *Tools*: Kedro + AIF360.  \n",
      "   *Deliverable*: Versioned pipeline + audit log.\n",
      "\n",
      "4. **Model Training & Fairness Constraints**  \n",
      "   *Tools*: Scikit‑learn + Fairlearn.  \n",
      "   *Deliverable*: Model Card + fairness metrics.\n",
      "\n",
      "5. **Evaluation & Regulatory Review**  \n",
      "   *Tools*: Evidently AI + Basel III stress‑test scripts.  \n",
      "   *Deliverable*: Evaluation Report + risk assessment.\n",
      "\n",
      "6. **Deployment & Monitoring**  \n",
      "   *Tools*: Docker + Seldon + Evidently.  \n",
      "   *Deliverable*: Production API + monitoring dashboards.\n",
      "\n",
      "7. **Post‑Deployment Review**  \n",
      "   *Frequency*: Quarterly.  \n",
      "   *Deliverable*: Model Review Report + updated Model Card.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Takeaways\n",
      "\n",
      "- **Integrate fairness and compliance from day one** – they are not add‑ons but core design constraints.  \n",
      "- **Document everything** – model cards, data sheets, audit logs, and governance decisions are mandatory for regulatory audits.  \n",
      "- **Automate monitoring** – fairness metrics should be tracked in real time, with alerts for drift.  \n",
      "- **Governance is continuous** – involve risk, compliance, and ethics boards throughout the model lifecycle.  \n",
      "\n",
      "By following this CRISP‑DM outline with embedded fairness and compliance checkpoints, your bank can build a robust, auditable credit‑risk model that meets both business objectives and regulatory expectations.\n",
      "--------------------\n",
      "推理耗时: 189.47 秒 (如果 VRAM 成功，应该也很快)\n",
      "卸载 20B 模型...\n",
      "20B 模型已卸载。\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 开始测试 GPT-OSS 20B (大脑) ---\")\n",
    "print(\"加载模型 (尝试全 VRAM)...\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    llm_brain = Llama(\n",
    "        model_path=MODEL_PATH_BRAIN,\n",
    "        n_gpu_layers=-1,  # -1 = 尝试将所有层加载到 GPU (全 VRAM)\n",
    "        n_ctx=4096,\n",
    "        verbose=True\n",
    "    )\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"模型加载完毕，耗时: {load_time:.2f} 秒\")\n",
    "\n",
    "    # 测试推理 (规划任务)\n",
    "    prompt = \"Outline the CRISP-DM steps for a bank credit risk model, focusing on fairness and compliance.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a senior data science manager at a bank.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    start_time = time.time()\n",
    "    output = llm_brain.create_chat_completion(messages=messages)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\n--- 20B '大脑' 模型输出 ---\")\n",
    "    print(output['choices'][0]['message']['content'])\n",
    "    print(\"--------------------\")\n",
    "    print(f\"推理耗时: {inference_time:.2f} 秒 (如果 VRAM 成功，应该也很快)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"加载 20B 模型时出错: {e}\")\n",
    "    print(\"⚠️ 警告：20B 模型可能无法完全载入 12GB VRAM。请查看下面的 '重要提示'。\")\n",
    "\n",
    "finally:\n",
    "    # 卸载模型\n",
    "    if 'llm_brain' in locals():\n",
    "        print(\"卸载 20B 模型...\")\n",
    "        del llm_brain\n",
    "        gc.collect()\n",
    "        print(\"20B 模型已卸载。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679b8c3a-95c1-43a0-90de-272e9d2e0c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
